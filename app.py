
# --- Gerekli KÃ¼tÃ¼phaneler ---
import streamlit as st
import google.generativeai as genai
from sentence_transformers import SentenceTransformer
import pandas as pd
import numpy as np
import os
# Hata tÃ¼rÃ¼nÃ¼ iÃ§e aktarÄ±yoruz
from streamlit.errors import StreamlitAPIException, StreamlitSecretNotFoundError
# KosinÃ¼s BenzerliÄŸi iÃ§in
from sklearn.metrics.pairwise import cosine_similarity 

# --- 1. Ä°lk Kurulum ve Modelleri YÃ¼kleme ---
@st.cache_resource
def load_models_and_data():
    print("Loading models and data...")
    
    # KullandÄ±ÄŸÄ±mÄ±z en son doÄŸru model
    local_model = SentenceTransformer('all-mpnet-base-v2') 
    
    # KÃ¼tÃ¼phanemizi (df_final) yÃ¼kle
    try:
        df_final = pd.read_pickle("my_rag_library_local.pkl") 
    except FileNotFoundError:
        st.error("KÃ¼tÃ¼phane dosyasÄ± (my_rag_library_local.pkl) bulunamadÄ±.")
        return None, None, None
    
    # Gemini API'yi yapÄ±landÄ±r
    try:
        api_key = st.secrets["GOOGLE_API_KEY"]
        genai.configure(api_key=api_key)
        
    except (KeyError, StreamlitSecretNotFoundError, StreamlitAPIException):
        try:
            api_key = os.environ.get('GOOGLE_API_KEY')
            if not api_key:
                raise ValueError("GOOGLE_API_KEY not found in os.environ")
            genai.configure(api_key=api_key)
        except Exception as e:
            st.error(f"API Key yapÄ±landÄ±rÄ±lamadÄ± (os.environ). Hata: {e}")
            return None, None, None
    
    # Ã‡alÄ±ÅŸtÄ±ÄŸÄ±nÄ± bildiÄŸimiz model
    generative_model = genai.GenerativeModel('models/gemini-2.5-flash') 
    
    print("Models and data loaded.")
    return local_model, generative_model, df_final

# --- 2. RAG FonksiyonlarÄ± (KosinÃ¼s BenzerliÄŸi ile) ---
def find_best_passages_local(query, dataframe, local_model, top_k=3):
    query_embedding = local_model.encode(query)
    doc_embeddings = np.stack(dataframe['Embeddings'].to_numpy())
    similarities = cosine_similarity(query_embedding.reshape(1, -1), doc_embeddings)[0]
    top_indices = np.argsort(similarities)[-top_k:][::-1]
    return "\n---\n".join(dataframe.iloc[top_indices]['description'].tolist())

def generate_answer(query, dataframe, local_model, generative_model):
    context = find_best_passages_local(query, dataframe, local_model)
    prompt = f"""
    You are a helpful bot. Answer the user's QUESTION based ONLY on the CONTEXT provided.
    If the answer is not in a context, say "I couldn't find an answer in the provided documents."
    CONTEXT: {context}
    QUESTION: {query}
    ANSWER:
    """
    try:
        response = generative_model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Cevap Ã¼retme sÄ±rasÄ±nda bir hata oluÅŸtu: {e}"

# --- 3. Streamlit ArayÃ¼z Kodu ---
st.title("ğŸ“š RAG Bilgi AsistanÄ±")
st.caption("SQuAD veri setini kullanan RAG tabanlÄ± bir chatbot demosu")

local_model, generative_model, df_final = load_models_and_data()

if local_model and generative_model and (df_final is not None):
    user_query = st.text_input("KÃ¼tÃ¼phaneye bir soru sorun:", placeholder="Which NFL team represented the AFC at Super Bowl 50?")
    if st.button("GÃ¶nder"):
        if user_query:
            with st.spinner("CevabÄ±nÄ±z aranÄ±yor..."):
                answer = generate_answer(user_query, df_final, local_model, generative_model)
                st.success("Cevap:")
                st.write(answer)
        else:
            st.warning("LÃ¼tfen bir soru girin.")
